{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc1d5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from pypdf) (4.10.0)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-4.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22fc6507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0\n",
      "  Downloading langsmith-0.1.50-py3-none-any.whl (115 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain_community) (6.0.1)\n",
      "Collecting langchain-core<0.2.0,>=0.1.45\n",
      "  Downloading langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain_community) (1.24.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain_community) (2.31.0)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.9.5-cp38-cp38-win_amd64.whl (373 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain_community) (2.0.29)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-win_amd64.whl (50 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp38-cp38-win_amd64.whl (77 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.45->langchain_community) (23.2)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.45->langchain_community) (2.7.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.45->langchain_community) (2.4)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.1-cp38-none-win_amd64.whl (138 kB)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.45->langchain_community) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.45->langchain_community) (4.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.45->langchain_community) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: orjson, mypy-extensions, multidict, frozenlist, yarl, typing-inspect, tenacity, marshmallow, langsmith, jsonpatch, async-timeout, aiosignal, langchain-core, dataclasses-json, aiohttp, langchain-community\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 dataclasses-json-0.6.4 frozenlist-1.4.1 jsonpatch-1.33 langchain-community-0.0.34 langchain-core-0.1.45 langsmith-0.1.50 marshmallow-3.21.1 multidict-6.0.5 mypy-extensions-1.0.0 orjson-3.10.1 tenacity-8.2.3 typing-inspect-0.9.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ec6e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.28 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain-text-splitters) (0.1.45)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (8.2.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.7.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (23.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (0.1.50)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.31.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (3.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (4.10.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.18.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nihal\\miniconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (1.26.7)\n",
      "Installing collected packages: langchain-text-splitters\n",
      "Successfully installed langchain-text-splitters-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc3839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain-google-genai (from versions: none)\n",
      "ERROR: No matching distribution found for langchain-google-genai\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7791c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cf7db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = PyPDFLoader(r\"C:\\Users\\Nihal\\OneDrive\\Desktop\\innomatics\\RAG\\rag.pdf\")\n",
    "pages=df.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e440d2a-244f-4a43-b120-32cb74d31050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nihal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8beaabc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 568, which is longer than the specified 500\n",
      "Created a chunk of size 506, which is longer than the specified 500\n",
      "Created a chunk of size 633, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500,chunk_overlap=100)\n",
    "\n",
    "chunks=text_splitter.split_documents(pages)\n",
    "\n",
    "print(len(chunks))\n",
    "\n",
    "print(type(chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46e8ba4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Preprint.\\n\\nUnder review.\\n\\nLeave No Context Behind:\\nEfficient Infinite Context Transformers with Infini-attention\\nTsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\\nGoogle\\ntsendsuren@google.com\\nAbstract\\nThis work introduces an efficient method to scale Transformer-based Large\\nLanguage Models (LLMs) to infinitely long inputs with bounded memory\\nand computation.\\n\\nA key component in our proposed approach is a new at-\\ntention technique dubbed Infini-attention.', metadata={'source': 'C:\\\\Users\\\\Nihal\\\\OneDrive\\\\Desktop\\\\innomatics\\\\RAG\\\\rag.pdf', 'page': 0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0609ec1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_google_genai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_google_genai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleGenerativeAIEmbeddings\n\u001b[0;32m      2\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m GoogleGenerativeAIEmbeddings(google_api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIzaSyDeFmoBbE6wNDKftGcF0mowbgzhC5HjzUw\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/embedding-001\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_google_genai'"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=\"AIzaSyDeFmoBbE6wNDKftGcF0mowbgzhC5HjzUw\", model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ba1ce04-bc44-475e-886a-9a0f6b67136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement langchain-google-genai (from versions: none)\n",
      "ERROR: No matching distribution found for langchain-google-genai\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1660abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the chunks in vector store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Embed each chunk and load it into the vector store\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"./chroma_db_\")\n",
    "\n",
    "# Persist the database on drive\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "707b23ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a Connection with the ChromaDB\n",
    "db_connection = Chroma(persist_directory=\"./chroma_db_\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d405b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.vectorstores.VectorStoreRetriever'>\n"
     ]
    }
   ],
   "source": [
    "# Converting CHROMA db_connection to Retriever Object\n",
    "retriever = db_connection.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "print(type(retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49cfc26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cddef651",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    # System Message Prompt Template\n",
    "    SystemMessage(content=\"\"\"You are a Helpful AI Bot. \n",
    "    Your task is to provide assistance based on the context given by the user. \n",
    "    Make sure your answers are relevant and helpful.\"\"\"),\n",
    "    # Human Message Prompt Template\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Answer the question based on the given context.\n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: \n",
    "    {question}\n",
    "    \n",
    "    Answer: \"\"\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35fd5fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(google_api_key=\"AIzaSyDeFmoBbE6wNDKftGcF0mowbgzhC5HjzUw\", model=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da907f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72ade762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | chat_template\n",
    "    | chat_model\n",
    "    | output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1abdcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## Gating Score Visualization Explained\\n\\nBased on the context you provided, gating score visualization refers to the graphical representation of the gating scores (sigmoid(β)) within the compressive memory of the Infini-attention model.  This visualization helps analyze the behavior of different attention heads within the model.\\n\\n**Key Points:**\\n\\n* **Figure 3** in the context specifically showcases this visualization.\\n* The gating score, represented by sigmoid(β), determines the degree to which information is allowed to pass through the compressive memory. \\n* **Two types of heads** are identified through this visualization:\\n    * **Specialized heads:** These have gating scores near 0 or 1, indicating they either block information or allow it to pass through completely.\\n    * **Mixer heads:** These have gating scores close to 0.5, suggesting they allow a mixture of information to pass through.\\n\\n**Significance:**\\n\\nVisualizing gating scores provides valuable insights into the inner workings of the Infini-attention model and how it processes information.  By understanding the behavior of different attention heads, researchers can further optimize the model's performance and efficiency. \\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"what is gating score visualization\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d52bac6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Gating Score Visualization Explained\n",
       "\n",
       "Based on the context you provided, gating score visualization refers to the graphical representation of the gating scores (sigmoid(β)) within the compressive memory of the Infini-attention model.  This visualization helps analyze the behavior of different attention heads within the model.\n",
       "\n",
       "**Key Points:**\n",
       "\n",
       "* **Figure 3** in the context specifically showcases this visualization.\n",
       "* The gating score, represented by sigmoid(β), determines the degree to which information is allowed to pass through the compressive memory. \n",
       "* **Two types of heads** are identified through this visualization:\n",
       "    * **Specialized heads:** These have gating scores near 0 or 1, indicating they either block information or allow it to pass through completely.\n",
       "    * **Mixer heads:** These have gating scores close to 0.5, suggesting they allow a mixture of information to pass through.\n",
       "\n",
       "**Significance:**\n",
       "\n",
       "Visualizing gating scores provides valuable insights into the inner workings of the Infini-attention model and how it processes information.  By understanding the behavior of different attention heads, researchers can further optimize the model's performance and efficiency. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f30bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2fb8c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Memory Retrieval and Update with Compressive Memory:\\n\\nBased on the context you provided, it seems like the approach discussed uses a **compressive memory** system for efficient memory retrieval and update. Let\\'s break down how it works:\\n\\n**Memory Retrieval:**\\n\\n* **Traditional Attention Mechanisms:** In standard attention mechanisms, the model attends to relevant parts of the input sequence to extract information. However, these mechanisms typically discard the \"key\" and \"value\" states after attention calculation, leading to a loss of potentially useful information.\\n* **Compressive Memory Approach:** This approach stores the old \"key-value\" (KV) states from attention calculations in a compressive memory, instead of discarding them. Later, when processing new sequences, the model can retrieve relevant information from the memory using the current \"query\" state. This retrieval is similar to the attention mechanism, allowing the model to access and utilize past information efficiently.\\n\\n**Memory Update:**\\n\\n* **Compressive Memory Structure:** Unlike traditional memory systems that grow with input size, compressive memory maintains a fixed size. This bounded nature ensures efficient storage and computation, even with long sequences. \\n* **Adding New Information:** When new information needs to be stored, the parameters of the compressive memory are updated. The update process aims to ensure that the new information can be accurately retrieved later using the query mechanism.\\n\\n**Benefits:**\\n\\n* **Efficiency:** The fixed size of compressive memory leads to bounded storage and computational costs, making it suitable for handling long sequences efficiently.\\n* **Information Reuse:** By storing and retrieving past information, the model can leverage a broader context for improved understanding and performance. \\n\\n**Additional Techniques:**\\n\\nThe context also mentions:\\n\\n* **System-level optimization:** Techniques like those proposed by Dao et al. (2022) and Liu et al. (2023) can further improve efficiency by leveraging specific hardware architectures for attention computation.\\n* **Linear attention mechanisms:** The memory update and retrieval processes can be cast as linear attention mechanisms, allowing the use of stable training techniques from related methods.\\n\\n**In summary, the approach described utilizes a compressive memory system to efficiently store and retrieve information, enabling the model to leverage past context for improved performance while maintaining computational efficiency.** \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"Can you please tell me about meory retival and memory update\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32ccb36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Memory Retrieval and Update with Compressive Memory:\n",
       "\n",
       "Based on the context you provided, it seems like the approach discussed uses a **compressive memory** system for efficient memory retrieval and update. Let's break down how it works:\n",
       "\n",
       "**Memory Retrieval:**\n",
       "\n",
       "* **Traditional Attention Mechanisms:** In standard attention mechanisms, the model attends to relevant parts of the input sequence to extract information. However, these mechanisms typically discard the \"key\" and \"value\" states after attention calculation, leading to a loss of potentially useful information.\n",
       "* **Compressive Memory Approach:** This approach stores the old \"key-value\" (KV) states from attention calculations in a compressive memory, instead of discarding them. Later, when processing new sequences, the model can retrieve relevant information from the memory using the current \"query\" state. This retrieval is similar to the attention mechanism, allowing the model to access and utilize past information efficiently.\n",
       "\n",
       "**Memory Update:**\n",
       "\n",
       "* **Compressive Memory Structure:** Unlike traditional memory systems that grow with input size, compressive memory maintains a fixed size. This bounded nature ensures efficient storage and computation, even with long sequences. \n",
       "* **Adding New Information:** When new information needs to be stored, the parameters of the compressive memory are updated. The update process aims to ensure that the new information can be accurately retrieved later using the query mechanism.\n",
       "\n",
       "**Benefits:**\n",
       "\n",
       "* **Efficiency:** The fixed size of compressive memory leads to bounded storage and computational costs, making it suitable for handling long sequences efficiently.\n",
       "* **Information Reuse:** By storing and retrieving past information, the model can leverage a broader context for improved understanding and performance. \n",
       "\n",
       "**Additional Techniques:**\n",
       "\n",
       "The context also mentions:\n",
       "\n",
       "* **System-level optimization:** Techniques like those proposed by Dao et al. (2022) and Liu et al. (2023) can further improve efficiency by leveraging specific hardware architectures for attention computation.\n",
       "* **Linear attention mechanisms:** The memory update and retrieval processes can be cast as linear attention mechanisms, allowing the use of stable training techniques from related methods.\n",
       "\n",
       "**In summary, the approach described utilizes a compressive memory system to efficiently store and retrieve information, enabling the model to leverage past context for improved performance while maintaining computational efficiency.** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f246d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
